{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization Exercise\n",
        "\n",
        "Tokenization is the process of breaking a text into smaller units called tokens. Tokens can be words, phrases, or punctuation marks."
      ],
      "metadata": {
        "id": "deC8TPdWyiit"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LgcmavfWygB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7c4491-ff51-49f6-865b-ae180ebcf5e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# EXERCISE: Tokenize a text using the Python nltk library\n",
        "# Expected output: ['This', 'is', 'a', 'sentence']\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize('This is a sentence')"
      ],
      "metadata": {
        "id": "Jv9LK4AHyqiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6557cd18-db8c-44ff-cae9-64bacb0cff10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'sentence']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# EXERCISE: Look up how to tokenize the sentence below using NLTK. The imports \n",
        "# are done for you. Does the NLTK tokenizer handle \"N.Y.C.\" correctly?\n",
        "#\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = \"Let's go to N.Y.C. for the weekend.\""
      ],
      "metadata": {
        "id": "Hk31wUfjyq95"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(s)"
      ],
      "metadata": {
        "id": "QwESCvB1yv-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5434b937-4e60-4389-b33e-5b4621f885c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', \"'s\", 'go', 'to', 'N.Y.C', '.', 'for', 'the', 'weekend', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Task"
      ],
      "metadata": {
        "id": "lLrZLwC4zD2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# EXERCISE:\n",
        "# 1) Tokenize the following text\n",
        "# 2) Iterate through the tokens to check whether there's a currency symbol.\n",
        "# 3) If there is, and the currency label is followed by a number, print\n",
        "#    both the symbol and the number.\n",
        "# \n",
        "# Look through https://spacy.io/api/token#attributes on how to check whether\n",
        "# a token is a currency symbol or a number.\n",
        "#\n",
        "# Expected output: \"$20\".\n",
        "s = \"He didn't want to pay $20 for this book.\"\n",
        "doc = nlp(s)"
      ],
      "metadata": {
        "id": "HcR4aF2-ywY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp= spacy.load('en_core_web_sm')\n",
        "s = \"He didn't want to pay $20 for this book.\"\n",
        "doc = nlp(s)\n",
        "for i in doc:\n",
        "  if i.is_currency or i.like_num:\n",
        "    print(i.text)\n"
      ],
      "metadata": {
        "id": "OYqG3qDVzIAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42440b6-d151-41ce-c72f-2bc2df608f21"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EW1-PYpwHRDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}